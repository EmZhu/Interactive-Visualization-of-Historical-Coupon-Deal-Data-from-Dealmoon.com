{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from time import sleep\n",
    "\n",
    "import unicodecsv as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawling(url, category, debug = False):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    master = [['Category', 'Title', 'Description', 'PostedTime', 'NumberOfComments', 'NumberOfBookmarks']]\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        elements = driver.find_elements_by_class_name('mlist')\n",
    "\n",
    "        for element in elements:\n",
    "            temp = [category]\n",
    "\n",
    "            #------ find title of deal\n",
    "            try:\n",
    "                title = element.find_element_by_tag_name('h1').find_element_by_tag_name('a').text\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                title = element.find_element_by_tag_name('h2').find_element_by_tag_name('a').text\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            temp.append(title)\n",
    "\n",
    "\n",
    "            #------ find description of deal\n",
    "            des = ''\n",
    "            des_elements = element.find_element_by_class_name('mbody').find_elements_by_tag_name('li')\n",
    "            \n",
    "            if len(des_elements) == 0:\n",
    "                des_elements = element.find_element_by_class_name('mbody').find_elements_by_tag_name('p')\n",
    "            \n",
    "            if len(des_elements) == 0:\n",
    "                des = element.find_element_by_class_name('mbody').find_element_by_tag_name('div').text\n",
    "                \n",
    "            if len(des_elements) == 0 and len(des) == 0:\n",
    "                print '===DESCRIPTION NOT FOUND'\n",
    "                print 'deal skipped'\n",
    "                continue\n",
    "            \n",
    "            for des_element in des_elements:\n",
    "                des += des_element.text\n",
    "                des += '\\n '\n",
    "                    \n",
    "            temp.append(des)\n",
    "            \n",
    "\n",
    "\n",
    "            #------ find time the deal was posted\n",
    "            try:\n",
    "                time = element.find_element_by_class_name('date').text\n",
    "                temp.append(time[0:-4]) #strip away 'Posted' and 'ago'\n",
    "            except Exception as e:\n",
    "                print '===TIME NOT FOUND'\n",
    "                print '===deal skipped'\n",
    "                continue\n",
    "\n",
    "\n",
    "            #------ find number of comments for the deal\n",
    "            try:\n",
    "                num_comments = element.find_element_by_class_name('pl_btn').find_element_by_tag_name('a').text\n",
    "                # If there are more than 1 comment, \"Comment\" is written as \"Comments\"\n",
    "                if len(num_comments) > 11:\n",
    "                    temp.append(num_comments[10:-1])\n",
    "                else:\n",
    "                    temp.append(num_comments[9:-1])\n",
    "            except Exception as e:\n",
    "                temp.append('0')\n",
    "                pass\n",
    "\n",
    "\n",
    "            #------ find number of comments for the deal\n",
    "            try:\n",
    "                num_comments = element.find_element_by_class_name('fav_btn').find_element_by_tag_name('em').text\n",
    "                temp.append(num_comments[1:-1])\n",
    "            except Exception as e:\n",
    "                temp.append('0')\n",
    "                pass\n",
    "\n",
    "            #------ append to master list\n",
    "            master.append(temp)\n",
    "\n",
    "\n",
    "        try:\n",
    "            load = driver.find_element_by_xpath(\"//a[@title='Next']\")\n",
    "\n",
    "            # See if the last page has been reached\n",
    "            page_num = driver.find_element_by_class_name('pages').find_element_by_class_name('current').text\n",
    "            \n",
    "            if page_num == u'100':\n",
    "                print 'Last page reached'\n",
    "                break\n",
    "            else:\n",
    "                load.click()          \n",
    "        except:\n",
    "            print \"===Can't go to the next page\"\n",
    "            break \n",
    "            \n",
    "        if debug == True:\n",
    "            break\n",
    "            \n",
    "    return master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveCSV(filename, data):\n",
    "    with open(filename, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Crawl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_categories = ['Clothing', 'Beauty', 'Nutrition', 'Baby', 'Home', 'Electronics', 'Travel', 'Finance']\n",
    "all_urls = ['http://www.dealmoon.com/Clothing-Jewelry-Bags',\n",
    "           'http://www.dealmoon.com/Beauty',\n",
    "           'http://www.dealmoon.com/Nutrition-Supplements',\n",
    "           'http://www.dealmoon.com/Baby',\n",
    "           'http://www.dealmoon.com/Home-Garden',\n",
    "           'http://www.dealmoon.com/Electronics',\n",
    "           'http://www.dealmoon.com/Travel',\n",
    "           'http://www.dealmoon.com/Finance']\n",
    "\n",
    "for i in range(len(all_categories)):\n",
    "    data = crawling(all_urls[i], all_categories[i])\n",
    "    print len(data)\n",
    "    filename = all_categories[i] + '.csv'\n",
    "    saveCSV(filename, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
